{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Picsell-ia/training/blob/master/Classification_TF2.ipynb)\n",
    "\n",
    "We assume that you have a working python3.6+ installation and tensorflow 2.x installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install picsellia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up your Picsell client\n",
    "\n",
    "First let's import Tensorflow and the Picsell.ia sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from picsellia import Client\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the name to your soon to come classification model and put your tokens here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = \"api_token\"\n",
    "project_token = \"project_token\" \n",
    "model_name = \"model_name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to initialize our client so we can communicate with the platform. We create a new network because we will do transfer learning on MobileNetV2 downloaded with tensorflow 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clt = Client(api_token=api_token)\n",
    "clt.checkout_project(project_token=project_token)\n",
    "clt.create_network(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the instance is created we can download the images and annotations and generate a label map.\n",
    "The label map is just here to map the string value of the label to a more convenient label id.\n",
    "\n",
    "The ```train_test_split()``` method is smartly splitting our data in two sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clt.dl_annotations()\n",
    "clt.generate_labelmap()\n",
    "clt.train_test_split()\n",
    "clt.dl_pictures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "\n",
    "## Converting data into serialized TFRecord files\n",
    "\n",
    "We want to serialize those images and labels inside a ```TFRecord``` format file. By doing so the data will be way more efficiently read by tensorflow. \n",
    "\n",
    "In order to do this we need to generate a ```tf.Example``` for each image which stores the image and its label as a protobuf, then we serialize and write those ```tf.Example``` objects inside the ```TFRecord``` file.\n",
    "\n",
    "First we create some shortcut functions to wrap the features messages. Those functions convert standard TensorFlow types to a ```tf.Example```-compatible ```tf.train.Feature``` object. In our case we just want to store the encoded image and the label id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create our .record files from there. \n",
    "\n",
    "To do so, we define a new function which will iterate for each set through each image and generate a ```tf.Example``` message that we'll write inside our file. \n",
    "\n",
    "We use the ```clt.tf_vars_generator``` method from the sdk to retrieve the data before converting them into the ```tf.Example``` message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_record_files(label_map, record_dir, tfExample_generator):\n",
    "    datasets = [\"train\", \"eval\"]    \n",
    "    for dataset in datasets:\n",
    "        output_path = os.path.join(record_dir, dataset+\".record\")\n",
    "        writer = tf.io.TFRecordWriter(output_path)\n",
    "        for variables in tfExample_generator(label_map, ensemble=dataset, annotation_type=\"classification\"):\n",
    "            (width, height, filename, encoded_jpg, image_format, \n",
    "                classes_text, classes) = variables\n",
    "\n",
    "            tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'image/encoded': _bytes_feature(encoded_jpg),\n",
    "                'image/object/class/label': _int64_feature(classes[0]-1)\n",
    "                }))\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "    \n",
    "        writer.close()\n",
    "        print('Successfully created the TFRecords: {}'.format(output_path))\n",
    "\n",
    "label_map = {v:int(k) for k,v in clt.label_map.items()}\n",
    "create_record_files(label_map=label_map, record_dir=clt.record_dir, \n",
    "                    tfExample_generator=clt.tf_vars_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our input pipeline\n",
    "\n",
    "Now that our data are saved in an efficient format we want to load them as a ```tf.Data.Dataset``` object.\n",
    "\n",
    "We have to define a feature_description dictionnary that follows the same structure as the one used to generate the ```tf.Example```.\n",
    " With this dictionnary we can define a parser for the ```tf.Example```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "      'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "      'image/object/class/label': tf.io.FixedLenFeature([], tf.int64, default_value=0)}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.Example` proto using the dictionary above.\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the ```tf.Data.dataset``` objects now by mapping the parser to the raw datasets !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(os.path.join(clt.record_dir,\"train.record\"))\n",
    "train_dataset = raw_dataset.map(_parse_function)\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(os.path.join(clt.record_dir,\"eval.record\"))\n",
    "eval_dataset = raw_dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset objects we want to do some pre-processing on them. \n",
    "\n",
    "For the label we will simply one_hot encode them. The images require a bit more attention. We will decode them, then resize them according to the size of the mobilenet_v2 model base input. Then we'll use the quite convenient ```mobilenet_v2.preprocess_input()``` function that cast the type to ```tf.float32``` and scale the pixels between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "def map_img_label(example_proto):\n",
    "    img = tf.io.decode_jpeg(example_proto[\"image/encoded\"], channels=3)\n",
    "    img = tf.image.resize(img, (224,224))\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    label = example_proto[\"image/object/class/label\"]\n",
    "    label = tf.one_hot(label, depth=2)\n",
    "    return (img,label)\n",
    "    \n",
    "train_set = train_dataset.map(map_img_label)\n",
    "eval_set = eval_dataset.map(map_img_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to shuffle and batch our datasets. With a ```tf.Data.dataset``` it's fairly simple. We just need to apply the corresponding method with some arguments, namely the batch size and the buffer size for the shuffling.\n",
    "\n",
    "We define some arbitrary values then apply the methods to our datasets. We do not use the ```repeat()``` method of a dataset because we want our epoch to end when the whole dataset is exhausted. \n",
    "\n",
    "If we added this method to both datasets, we would need to pass a ```steps_per_epoch``` and ``validation_steps`` to the ``fit`` method of our model when starting the training. Indeed, Tensorflow would not be able to know when to stop an epoch since the dataset will be infinitely repeating itself. \n",
    "\n",
    "At this stage we could add some data augmentation by mapping functions to the dataset. However we will not do it in this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "SHUFFLE_BUFFER_SIZE = 50\n",
    "\n",
    "train_set = train_set.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_set = eval_set.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation and training\n",
    "\n",
    "## Model definition\n",
    "\n",
    "Now that our input pipeline is built it's time to define our model. As said earlier we are going to do some transfer learning on the MobileNetV2 model. First let's import some keras functions and the MobileNetV2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will be made up of two sub-models. The first part will be the MobileNetV2 model with all of its layers frozen and we will plug on top of it a little headModel defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "    input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "model = Model(inputs = baseModel.input, outputs = headModel)\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the summary of our model and see all the different layers as well as the number of trainable/non-trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "\n",
    "We first define some arbitrary hyperparameters and a specific optimizer.\n",
    "\n",
    "The next step is to compile our model. It's here that we can set the loss, metrics and optimizer chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 100\n",
    "\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Let's start the training by using the ``fit`` method of our model. As arguments we simply specify a ``tf.Data`` train and validation sets and the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "History = model.fit(train_set,\n",
    "    validation_data=eval_set,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving\n",
    "\n",
    "### Training logs\n",
    "\n",
    "By default the fit method of a model returns a ```tf.keras.callbacks.History``` object which has some base logs from the training. We want to send those logs to the platform to see them on the dashboard. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logs = {k:{\"step\": [str(e) for e in History.epoch], \"value\":[str(round(val, 3)) for val in v] } for k,v in History.history.items()}\n",
    "clt.send_logs(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create and send a dictionnary containing the logs in the right format for the platform to display them. \n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "We want to save a checkpoint of our model to, for example, continue the training later. To do this we want to create a ```Checkpoint``` object with our model and optimizer and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(optimizer=opt, model=model)\n",
    "checkpoint.save(os.path.join(clt.checkpoint_dir, \"model.ckpt\"))\n",
    "clt.send_checkpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Saving a checkpoint is nice but mostly useful for future trainings. To directly save the model we simply need to apply the ```save``` method to our model and specify the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(clt.exported_model_dir, save_format=\"tf\")\n",
    "clt.send_model(\"mask_classif/Classification_COVID_Ma/0/exported_model/\")\n",
    "clt.send_labelmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}